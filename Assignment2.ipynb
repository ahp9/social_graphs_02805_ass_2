{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, io, json, requests, zipfile, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# --- 1️⃣ Fetch performers folder from GitHub ---\n",
    "url = \"https://github.com/ahp9/social_graphs_02805_ass_2/archive/refs/heads/main.zip\"\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "DATA_DIR = \"social_graphs_02805_ass_2-main/performers\"\n",
    "\n",
    "# --- 2️⃣ Load LabMT sentiment dataset ---\n",
    "labmt_url = \"https://raw.githubusercontent.com/ahp9/social_graphs_02805_ass_2/main/Data_Set_S1.txt\"\n",
    "labmt = pd.read_csv(labmt_url, sep=\"\\t\", skiprows=3)\n",
    "word_to_scores = labmt.set_index(\"word\").to_dict(orient=\"index\")\n",
    "\n",
    "# --- 3️⃣ Define helper functions ---\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "LINK_RE = re.compile(r\"\\[\\[([^\\]|#]+)\\]\")\n",
    "INFOBOX_NAMES = (\n",
    "    \"Infobox musical artist\",\n",
    "    \"Infobox musical artist2\",\n",
    "    \"Infobox musical artist/Person\",\n",
    "    \"Infobox person\",\n",
    ")\n",
    "\n",
    "def file_to_title(fname): return fname.replace(\".txt\", \"\").replace(\"_\", \" \")\n",
    "def normalize(title): return re.sub(r\"[^a-z0-9]+\", \"\", title.lower())\n",
    "\n",
    "# --- Infobox extraction helpers ---\n",
    "def extract_infobox(text):\n",
    "    start_pat = r\"\\{\\{\\s*(?:\" + \"|\".join(map(re.escape, INFOBOX_NAMES)) + r\")\\b\"\n",
    "    m = re.search(start_pat, text, flags=re.IGNORECASE)\n",
    "    if not m: return None\n",
    "    i = m.start(); j = i; depth = 0\n",
    "    while j < len(text) - 1:\n",
    "        two = text[j:j+2]\n",
    "        if two == \"{{\": depth += 1; j += 2; continue\n",
    "        if two == \"}}\": depth -= 1; j += 2\n",
    "        if depth == 0: return text[i:j]\n",
    "        j += 1\n",
    "    return None\n",
    "\n",
    "def split_top_level_params(infobox):\n",
    "    first_bar = infobox.find(\"|\", infobox.find(\"{{\") + 2)\n",
    "    if first_bar == -1: return {}\n",
    "    body = infobox[first_bar:]\n",
    "    params, buf, depth, i, L = {}, [], 0, 0, len(body)\n",
    "\n",
    "    def flush_field(s):\n",
    "        s = s.lstrip(\"|\").strip()\n",
    "        if \"=\" in s:\n",
    "            k, v = s.split(\"=\", 1)\n",
    "            params[k.strip().casefold()] = v.strip()\n",
    "\n",
    "    while i < L:\n",
    "        two = body[i:i+2]\n",
    "        if two == \"{{\": depth += 1; buf.append(two); i += 2; continue\n",
    "        if two == \"}}\": depth = max(0, depth - 1); buf.append(two); i += 2; continue\n",
    "        if body[i:i+2] == \"\\n|\" and depth == 0:\n",
    "            flush_field(\"\".join(buf)); buf = []; i += 2; continue\n",
    "        buf.append(body[i]); i += 1\n",
    "    flush_field(\"\".join(buf))\n",
    "    return params\n",
    "\n",
    "def extract_genre_block_from_infobox(infobox_text):\n",
    "    params = split_top_level_params(infobox_text)\n",
    "    return params.get(\"genre\", \"\")\n",
    "\n",
    "LIST_WRAPPERS = (\"hlist\", \"flatlist\", \"ubl\", \"unbulleted list\", \"plainlist\", \"plain list\")\n",
    "\n",
    "def clean_genre_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"<!--.*?-->\", \"\", s, flags=re.DOTALL)\n",
    "    s = re.sub(r\"<br\\s*/?>\", \",\", s)\n",
    "    s = re.sub(r\"<[^>]+>\", \"\", s)\n",
    "    s = re.sub(r\"\\{\\{\\s*(?:\" + \"|\".join(LIST_WRAPPERS) + r\")\\s*\\|\", \"\", s)\n",
    "    s = s.replace(\"{{\", \"\").replace(\"}}\", \"\")\n",
    "    s = re.sub(r\"^\\s*\\*+\\s*\", \"\", s, flags=re.MULTILINE)\n",
    "    s = s.replace(\"\\n*\", \"\\n\")\n",
    "    return s\n",
    "\n",
    "def extract_genres(genre_block):\n",
    "    s = clean_genre_text(genre_block)\n",
    "    s = re.sub(r\"cite\\s+web[\\s\\S]*?(?=(\\[\\[|$))\", \"\", s)\n",
    "    s = re.sub(r\"\\b(?:url|title|publisher|access-date|work|last|first)\\s*=\\s*[^|}\\n]*\", \"\", s)\n",
    "    link_matches = re.findall(r\"\\[\\[([^\\]]+)\\]\\]\", s)\n",
    "    genres = []\n",
    "    for match in link_matches:\n",
    "        parts = match.split(\"|\")\n",
    "        genre = parts[-1].strip()\n",
    "        if genre and not any(x in genre for x in (\"http\", \"cite\", \"url\", \"access-date\")):\n",
    "            genres.append(genre)\n",
    "    out, seen = [], set()\n",
    "    for g in genres:\n",
    "        g2 = re.sub(r\"\\s+\", \" \", g).strip()\n",
    "        key = g2.casefold()\n",
    "        if key not in seen:\n",
    "            seen.add(key); out.append(g2)\n",
    "    return out\n",
    "\n",
    "def canonicalize_genre(g):\n",
    "    s = g.strip().replace(\"’\", \"'\").replace(\"&amp;\", \"&\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().casefold()\n",
    "    s = re.sub(r\"\\brock\\s*(?:'n'|’n’|n’|’n|n'|&|and)\\s*roll\\b\", \"rock and roll\", s)\n",
    "    s = re.sub(r\"\\bhip[-\\s]?hop\\b\", \"hip hop\", s)\n",
    "    s = re.sub(r\"\\brhythm and blues\\b\", \"r&b\", s)\n",
    "    return s.strip()\n",
    "\n",
    "# --- 4️⃣ Build the artist graph ---\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".txt\")]\n",
    "performers = [file_to_title(f) for f in files]\n",
    "norm2canon = {normalize(t): t for t in performers}\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "graph.add_nodes_from(performers)\n",
    "\n",
    "for fname in files:\n",
    "    src = file_to_title(fname)\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        text = fh.read()\n",
    "    graph.nodes[src][\"text\"] = text\n",
    "\n",
    "    infobox = extract_infobox(text)\n",
    "    if infobox:\n",
    "        genre_block = extract_genre_block_from_infobox(infobox)\n",
    "        genres = [canonicalize_genre(g) for g in extract_genres(genre_block)]\n",
    "        if genres:\n",
    "            graph.nodes[src][\"genres\"] = list(set(genres))\n",
    "\n",
    "    # find outgoing links\n",
    "    unique_targets = set()\n",
    "    for raw in LINK_RE.findall(text):\n",
    "        tgt_norm = normalize(raw)\n",
    "        if tgt_norm in norm2canon:\n",
    "            dst = norm2canon[tgt_norm]\n",
    "            if dst != src:\n",
    "                unique_targets.add(dst)\n",
    "    for dst in unique_targets:\n",
    "        graph.add_edge(src, dst)\n",
    "\n",
    "graph = graph.to_undirected()\n",
    "print(f\"Graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
    "\n",
    "# --- 5️⃣ Sentiment analysis ---\n",
    "def calculate_sentiment(tokens):\n",
    "    sentiment_score = 0\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token in word_to_scores:\n",
    "            sentiment_score += word_to_scores[token][\"happiness_average\"]\n",
    "            count += 1\n",
    "    return sentiment_score / count if count > 0 else 0\n",
    "\n",
    "for node in graph.nodes:\n",
    "    text = graph.nodes[node].get(\"text\", \"\")\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    sentiment = calculate_sentiment(tokens)\n",
    "    graph.nodes[node][\"sentiment\"] = sentiment\n",
    "\n",
    "sentiments = [graph.nodes[n][\"sentiment\"] for n in graph.nodes if graph.nodes[n][\"sentiment\"] is not None]\n",
    "mean_sent, median_sent = np.mean(sentiments), np.median(sentiments)\n",
    "variance_sent = np.var(sentiments)\n",
    "q25, q75 = np.percentile(sentiments, [25, 75])\n",
    "\n",
    "print(f\"Mean: {mean_sent:.2f}, Median: {median_sent:.2f}, Var: {variance_sent:.2f}\")\n",
    "\n",
    "# --- 6️⃣ Plot the sentiment histogram ---\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sentiments, bins=20, color=\"lightgray\", edgecolor=\"black\")\n",
    "ax.axvline(mean_sent, color=\"darkred\", linestyle=\"--\", linewidth=2, label=f\"Mean = {mean_sent:.2f}\")\n",
    "ax.axvline(median_sent, color=\"black\", linestyle=\"-\", linewidth=2, label=f\"Median = {median_sent:.2f}\")\n",
    "ax.set_xlabel(\"Sentiment\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Sentiment Histogram\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 7️⃣ (Optional) Save graph with genres + sentiment ---\n",
    "nx.write_gexf(graph, \"artist_graph_with_genres_sentiment.gexf\")\n",
    "print(\"✅ Saved artist_graph_with_genres_sentiment.gexf\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
